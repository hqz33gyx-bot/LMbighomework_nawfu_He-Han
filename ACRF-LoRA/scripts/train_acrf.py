"""
[START] AC-RF Training Script for Z-Image-Turbo

ç‹¬ç«‹çš„ Anchor-Constrained Rectified Flow è®­ç»ƒè„šæœ¬
ç”¨äº Z-Image-Turbo æ¨¡å‹çš„ LoRA å¾®è°ƒå®éªŒ

å…³é”®ç‰¹æ€§ï¼š
- ä¿æŒ Turbo æ¨¡å‹çš„ç›´çº¿åŠ é€Ÿç»“æ„
- åªåœ¨å…³é”®é”šç‚¹æ—¶é—´æ­¥è®­ç»ƒ
- ç›´æ¥å›å½’é€Ÿåº¦å‘é‡è€Œéé¢„æµ‹å™ªå£°
"""

import os
import sys
import math
import signal
import time
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../src"))

import torch
import torch.nn.functional as F
import argparse
from pathlib import Path
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed

from zimage_trainer.acrf_trainer import ACRFTrainer
from zimage_trainer.utils.zimage_utils import load_transformer
from zimage_trainer.networks.lora import LoRANetwork
from zimage_trainer.dataset.dataloader import create_dataloader
from zimage_trainer.utils.memory_optimizer import MemoryOptimizer
from zimage_trainer.utils.hardware_detector import HardwareDetector
from zimage_trainer.utils.snr_utils import compute_snr_weights, print_anchor_snr_weights
from zimage_trainer.losses import FrequencyAwareLoss, LatentStyleStructureLoss

import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨å±€ä¸­æ–­æ ‡å¿—
_interrupted = False

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨"""
    global _interrupted
    _interrupted = True
    logger.info("\n[STOP] æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œå°†åœ¨å½“å‰æ­¥éª¤å®Œæˆåä¿å­˜å¹¶é€€å‡º...")

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)


def parse_args():
    parser = argparse.ArgumentParser(description="AC-RF è®­ç»ƒè„šæœ¬")
    
    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument("--config", type=str, help="è¶…å‚æ•°é…ç½®æ–‡ä»¶è·¯å¾„ (.toml)")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--dit", type=str, help="Transformer æ¨¡å‹è·¯å¾„")
    parser.add_argument("--dataset_config", type=str, help="æ•°æ®é›†é…ç½®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="output/acrf", help="è¾“å‡ºç›®å½•")
    
    # AC-RF å‚æ•°
    parser.add_argument("--turbo_steps", type=int, default=10, help="Turbo æ­¥æ•°ï¼ˆé”šç‚¹æ•°é‡ï¼‰")
    parser.add_argument("--shift", type=float, default=3.0, help="æ—¶é—´æ­¥ shift å‚æ•°")
    parser.add_argument("--jitter_scale", type=float, default=0.02, help="é”šç‚¹æŠ–åŠ¨å¹…åº¦")
    
    # LoRA å‚æ•°
    parser.add_argument("--network_dim", type=int, default=8, help="LoRA rank")
    parser.add_argument("--network_alpha", type=float, default=4.0, help="LoRA alpha")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--optimizer_type", type=str, default="AdamW", choices=["AdamW", "AdamW8bit", "Adafactor"], help="ä¼˜åŒ–å™¨ç±»å‹")
    # Adafactor ç‰¹æœ‰å‚æ•°
    parser.add_argument("--adafactor_scale", action="store_true", help="Adafactor scale_parameter")
    parser.add_argument("--adafactor_relative", action="store_true", help="Adafactor relative_step")
    parser.add_argument("--adafactor_warmup", action="store_true", help="Adafactor warmup_init")
    
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    
    # LR Scheduler å‚æ•°
    parser.add_argument("--lr_scheduler", type=str, default="constant", 
        choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
        help="å­¦ä¹ ç‡è°ƒåº¦å™¨"
    )
    parser.add_argument("--lr_warmup_steps", type=int, default=0, help="Warmup æ­¥æ•°")
    parser.add_argument("--lr_num_cycles", type=int, default=1, help="Cosine è°ƒåº¦å™¨çš„å¾ªç¯æ¬¡æ•°")
    
    # Min-SNR åŠ æƒå‚æ•°ï¼ˆç»Ÿä¸€åº”ç”¨äºæ‰€æœ‰ loss æ¨¡å¼ï¼‰
    parser.add_argument("--snr_gamma", type=float, default=5.0, help="Min-SNR gamma (0=ç¦ç”¨, æ¨è5.0)")
    parser.add_argument("--snr_floor", type=float, default=0.1, help="Min-SNR ä¿åº•æƒé‡ (10æ­¥æ¨¡å‹å…³é”®å‚æ•°ï¼Œæ¨è0.1)")
    
    # æŸå¤±æƒé‡å‚æ•°
    parser.add_argument("--lambda_l1", type=float, default=1.0, help="Charbonnier/L1 Loss æƒé‡")
    parser.add_argument("--lambda_cosine", type=float, default=0.1, help="Cosine Loss æƒé‡")
    
    # é¢‘åŸŸæ„ŸçŸ¥æŸå¤± (å¼€å…³+æƒé‡+å­å‚æ•°)
    parser.add_argument("--enable_freq", action="store_true", help="å¯ç”¨é¢‘åŸŸæ„ŸçŸ¥æŸå¤±")
    parser.add_argument("--lambda_freq", type=float, default=0.3, help="é¢‘åŸŸæ„ŸçŸ¥ Loss æƒé‡")
    
    # é£æ ¼ç»“æ„æŸå¤± (å¼€å…³+æƒé‡+å­å‚æ•°)
    parser.add_argument("--enable_style", action="store_true", help="å¯ç”¨é£æ ¼ç»“æ„æŸå¤±")
    parser.add_argument("--lambda_style", type=float, default=0.3, help="é£æ ¼ç»“æ„ Loss æƒé‡")
    
    # L2 æŸå¤±ç‹¬ç«‹é‡‡æ ·é…ç½®ï¼ˆå…¨æ—¶é—´æ­¥éšæœºé‡‡æ ·ï¼Œä¸ä½¿ç”¨é”šç‚¹ï¼‰
    parser.add_argument("--lambda_mse", type=float, default=0.0, help="L2/MSE Loss æƒé‡ (0=ç¦ç”¨)")
    parser.add_argument("--mse_use_anchor", type=bool, default=False, help="L2 æ˜¯å¦ä½¿ç”¨é”šç‚¹ (False=å…¨æ—¶é—´æ­¥éšæœº)")
    
    # RAFT æ··åˆæ¨¡å¼å‚æ•° (åŒ batch æ··åˆé”šç‚¹æµ+è‡ªç”±æµ)
    parser.add_argument("--free_stream_ratio", type=float, default=0.3, help="è‡ªç”±æµæ¯”ä¾‹ (0.3=30%% å…¨æ—¶é—´æ­¥éšæœº)")
    parser.add_argument("--raft_mode", action="store_true", help="å¯ç”¨ RAFT åŒ batch æ··åˆæ¨¡å¼")
    
    # Latent Jitter: ç©ºé—´æŠ åŠ¨ (å‚ç›´äºæµçº¿æ–¹å‘ï¼ŒçœŸæ­£æ”¹å˜æ„å›¾çš„å…³é”®)
    # æ¨è 0.03-0.05ï¼Œé…åˆ target = x0 - x_t_perturbed
    parser.add_argument("--latent_jitter_scale", type=float, default=0.0, help="Latent ç©ºé—´æŠ åŠ¨å¹…åº¦ (0=ç¦ç”¨, æ¨è 0.03-0.05)")
    
    # é¢‘åŸŸæ„ŸçŸ¥ Loss å­å‚æ•°
    parser.add_argument("--alpha_hf", type=float, default=1.0, help="é«˜é¢‘å¢å¼ºæƒé‡")
    parser.add_argument("--beta_lf", type=float, default=0.2, help="ä½é¢‘é”å®šæƒé‡")
    parser.add_argument("--lf_magnitude_weight", type=float, default=0.0, help="ä½é¢‘å¹…åº¦çº¦æŸ")
    parser.add_argument("--downsample_factor", type=int, default=4, help="ä½é¢‘æå–é™é‡‡æ ·å› å­")
    
    # é£æ ¼ç»“æ„ Loss å­å‚æ•°
    parser.add_argument("--lambda_struct", type=float, default=1.0, help="ç»“æ„é”æƒé‡ (SSIM)")
    parser.add_argument("--lambda_light", type=float, default=0.5, help="å…‰å½±å­¦ä¹ æƒé‡ (Lé€šé“ç»Ÿè®¡)")
    parser.add_argument("--lambda_color", type=float, default=0.3, help="è‰²è°ƒè¿ç§»æƒé‡ (abé€šé“ç»Ÿè®¡)")
    parser.add_argument("--lambda_tex", type=float, default=0.5, help="è´¨æ„Ÿå¢å¼ºæƒé‡ (é«˜é¢‘L1)")
    
    # è®­ç»ƒæ§åˆ¶ (Epoch æ¨¡å¼)
    parser.add_argument("--num_train_epochs", type=int, default=10, help="è®­ç»ƒ Epoch æ•°")
    parser.add_argument("--save_every_n_epochs", type=int, default=1, help="ä¿å­˜é—´éš” (Epoch)")
    parser.add_argument("--output_name", type=str, default="zimage-lora", help="LoRA è¾“å‡ºæ–‡ä»¶å")
    
    # å…¼å®¹æ€§ä¿ç•™ (ä¼šè¢«è‡ªåŠ¨è¦†ç›–)
    parser.add_argument("--max_train_steps", type=int, default=None, help="æœ€å¤§è®­ç»ƒæ­¥æ•° (è‡ªåŠ¨è®¡ç®—)")
    parser.add_argument("--save_every_n_steps", type=int, default=None, help="ä¿å­˜é—´éš” (æ­¥æ•°)")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯")
    parser.add_argument("--mixed_precision", type=str, default="fp16", choices=["no", "fp16", "bf16"])
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    # é«˜çº§åŠŸèƒ½
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    parser.add_argument("--blocks_to_swap", type=int, default=0, 
        help="å°†å¤šå°‘ä¸ª transformer blocks äº¤æ¢åˆ° CPUï¼ŒèŠ‚çœæ˜¾å­˜ã€‚"
             "16Gæ˜¾å­˜å»ºè®®è®¾ä¸º 4-8ï¼Œ24Gæ˜¾å­˜å¯ä¸è®¾ç½®")
    
    # è‡ªåŠ¨ä¼˜åŒ–åŠŸèƒ½
    parser.add_argument("--auto_optimize", action="store_true", default=True, help="å¯ç”¨è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–")
    
    # æ•°æ®åŠ è½½å‚æ•°
    parser.add_argument("--enable_bucket", action="store_true", default=True, help="å¯ç”¨åˆ†æ¡¶ (æŒ‰åˆ†è¾¨ç‡åˆ†ç»„)")
    parser.add_argument("--disable_bucket", action="store_true", help="ç¦ç”¨åˆ†æ¡¶ (æ‰€æœ‰å›¾ç‰‡å¿…é¡»ç›¸åŒå°ºå¯¸)")
    
    # SDPA (Scaled Dot-Product Attention) å‚æ•°
    parser.add_argument("--attention_backend", type=str, default="sdpa", 
        choices=["sdpa", "flash", "_flash_3"], help="æ³¨æ„åŠ›åç«¯é€‰æ‹©")
    parser.add_argument("--enable_flash_attention", action="store_true", help="å¯ç”¨Flash Attention")
    parser.add_argument("--sdpa_optimize_level", type=str, default="auto",
        choices=["fast", "memory_efficient", "auto"], help="SDPAä¼˜åŒ–çº§åˆ«")
    parser.add_argument("--use_memory_efficient_attention", action="store_true", default=True, help="ä½¿ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›")
    parser.add_argument("--attention_dropout", type=float, default=0.0, help="æ³¨æ„åŠ›dropoutç‡")
    parser.add_argument("--force_deterministic", action="store_true", help="å¼ºåˆ¶ç¡®å®šæ€§è®¡ç®—")
    parser.add_argument("--sdpa_min_seq_length", type=int, default=512, help="SDPAæœ€å°åºåˆ—é•¿åº¦é˜ˆå€¼")
    parser.add_argument("--sdpa_batch_size_threshold", type=int, default=4, help="SDPAæ‰¹é‡å¤§å°é˜ˆå€¼")
    
    # Block Swapping (å—äº¤æ¢æŠ€æœ¯) å‚æ•°
    parser.add_argument("--block_swap_enabled", action="store_true", help="å¯ç”¨å—äº¤æ¢æŠ€æœ¯")
    parser.add_argument("--block_swap_block_size", type=int, default=256, help="å—äº¤æ¢å†…å­˜å—å¤§å°")
    parser.add_argument("--block_swap_cpu_buffer_size", type=int, default=1024, help="å—äº¤æ¢CPUç¼“å†²åŒºå¤§å° (MB)")
    parser.add_argument("--block_swap_swap_threshold", type=float, default=0.7, help="å—äº¤æ¢é˜ˆå€¼ (0.1-0.9)")
    parser.add_argument("--block_swap_swap_strategy", type=str, default="lru", choices=["fifo", "lru", "priority"], help="å—äº¤æ¢ç­–ç•¥")
    parser.add_argument("--block_swap_compression_enabled", action="store_true", help="å¯ç”¨å—äº¤æ¢å‹ç¼©")
    parser.add_argument("--block_swap_prefetch_enabled", action="store_true", help="å¯ç”¨å—äº¤æ¢é¢„å–")
    parser.add_argument("--activation_checkpoint_block_size", type=int, default=64, help="æ¿€æ´»æ£€æŸ¥ç‚¹å—å¤§å°")
    parser.add_argument("--memory_monitoring_enabled", action="store_true", help="å¯ç”¨å†…å­˜ç›‘æ§")
    parser.add_argument("--memory_swap_frequency", type=int, default=5, help="å†…å­˜äº¤æ¢é¢‘ç‡")
    parser.add_argument("--memory_pool_strategy", type=str, default="conservative",
        choices=["none", "conservative", "aggressive"], help="å†…å­˜æ± ç­–ç•¥")
    
    # æ–‡æœ¬åºåˆ—é•¿åº¦å‚æ•°
    parser.add_argument("--max_sequence_length", type=int, default=512, help="æ–‡æœ¬ç¼–ç å™¨æœ€å¤§åºåˆ—é•¿åº¦ (éœ€ä¸ç¼“å­˜æ—¶ä¸€è‡´)")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œè¯»å–å¹¶è¦†ç›–é»˜è®¤å€¼
    if args.config:
        import tomli
        with open(args.config, "rb") as f:
            config = tomli.load(f)
            
        # æ‰å¹³åŒ– config å­—å…¸ä»¥ä¾¿æ˜ å°„
        flat_config = {}
        for section in config.values():
            flat_config.update(section)
            
        # æ›´æ–° args (ä»…å½“å‘½ä»¤è¡ŒæœªæŒ‡å®šæ—¶ä½¿ç”¨ config å€¼ï¼Œæˆ–è€…ç›´æ¥è¦†ç›–ï¼Ÿé€šå¸¸å‘½ä»¤è¡Œä¼˜å…ˆçº§æ›´é«˜)
        # è¿™é‡Œæˆ‘ä»¬å®ç°ï¼šConfig è¦†ç›–é»˜è®¤å€¼ï¼Œå‘½ä»¤è¡Œè¦†ç›– Config
        
        # 1. è®¾ç½® Config ä¸­çš„å€¼
        for key, value in flat_config.items():
            # åªæœ‰å½“ args ä¸­å­˜åœ¨è¯¥å±æ€§ä¸”å‘½ä»¤è¡Œæœªæ˜¾å¼æŒ‡å®šï¼ˆè¿™é‡Œæ¯”è¾ƒéš¾åˆ¤æ–­æ˜¯å¦æ˜¾å¼æŒ‡å®šï¼Œ
            # ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾å¦‚æœ config æœ‰å€¼å°±ç”¨ config çš„ï¼Œé™¤é args æ˜¯ Noneï¼‰
            # æ›´ç¨³å¥çš„åšæ³•æ˜¯ï¼šargparse default è®¾ä¸º Noneï¼Œç„¶åæ‰‹åŠ¨å¤„ç† defaults
            if hasattr(args, key):
                setattr(args, key, value)
    
    # å†æ¬¡è§£æå‘½ä»¤è¡Œå‚æ•°ä»¥ç¡®ä¿å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æœ€é«˜ (éœ€è¦ç¨å¾®é‡æ„ï¼Œæˆ–è€…ç®€å•åœ°åªç”¨ config)
    # ç®€å•å®ç°ï¼šå¦‚æœæä¾›äº† configï¼Œå°±ç”¨ config çš„å€¼è¦†ç›– args çš„é»˜è®¤å€¼
    # ä½†è¿™æ ·å‘½ä»¤è¡Œå‚æ•°å°±æ— æ•ˆäº†ã€‚
    
    # æ›´å¥½çš„å®ç°ï¼š
    # 1. Parse args å¾—åˆ°å‘½ä»¤è¡Œå‚æ•°
    # 2. Load config
    # 3. å¦‚æœå‘½ä»¤è¡Œå‚æ•°æ˜¯é»˜è®¤å€¼ï¼Œä¸” config ä¸­æœ‰å€¼ï¼Œåˆ™ä½¿ç”¨ config çš„å€¼
    # ä½† argparse ä¸å®¹æ˜“åŒºåˆ†"é»˜è®¤å€¼"å’Œ"ç”¨æˆ·è¾“å…¥çš„å€¼"ã€‚
    
    # è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸å»ºè®®ï¼šå¦‚æœç”¨äº† --configï¼Œå°±ä¸»è¦ä¾èµ– configã€‚
    # æˆ–è€…ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ£€æŸ¥ sys.argv
    
    # è®©æˆ‘ä»¬é‡‡ç”¨æœ€ç®€å•çš„ç­–ç•¥ï¼šConfig æ–‡ä»¶ä½œä¸º"æ–°çš„é»˜è®¤å€¼"
    if args.config:
        # é‡æ–°è§£æï¼Œè¿™æ¬¡å°† config ä¸­çš„å€¼ä½œä¸º default
        import tomli
        with open(args.config, "rb") as f:
            config = tomli.load(f)
        
        defaults = {}
        for section in config.values():
            defaults.update(section)
            
        parser.set_defaults(**defaults)
        args = parser.parse_args() # å†æ¬¡è§£æï¼Œè¿™æ ·å‘½ä»¤è¡Œå‚æ•°ä¼šè¦†ç›– config (ä½œä¸º defaults)
        
    # éªŒè¯å¿…è¦å‚æ•°
    if not args.dit:
        parser.error("--dit is required (or set in config)")
    
    # dataset_config å¯é€‰ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨ä¸»é…ç½®æ–‡ä»¶
    if not args.dataset_config and args.config:
        args.dataset_config = args.config  # ä½¿ç”¨ä¸»é…ç½®æ–‡ä»¶ä¸­çš„ [dataset] éƒ¨åˆ†
        
    return args


def main():
    args = parse_args()
    
    # ç¡¬ä»¶æ£€æµ‹å’Œè‡ªåŠ¨ä¼˜åŒ–
    logger.info("[DETECT] æ­£åœ¨è¿›è¡Œç¡¬ä»¶æ£€æµ‹...")
    hardware_detector = HardwareDetector()
    hardware_detector.print_detection_summary()
    
    # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨ä¼˜åŒ–ï¼Œåˆ™åº”ç”¨ä¼˜åŒ–é…ç½®
    if args.auto_optimize:
            logger.info("[TARGET] å¯ç”¨è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–...")
            
            # å¦‚æœé…ç½®æ˜¯ç®€åŒ–é…ç½®ï¼Œåº”ç”¨è‡ªåŠ¨ä¼˜åŒ–
            if args.config:
                try:
                    # å°è¯•å¯¼å…¥tomliï¼ˆTOMLè§£æåº“ï¼‰
                    try:
                        import tomli
                        with open(args.config, "rb") as f:
                            config = tomli.load(f)
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰tomliï¼Œä½¿ç”¨tomllibï¼ˆPython 3.11+å†…ç½®ï¼‰
                        import tomllib
                        with open(args.config, "rb") as f:
                            config = tomllib.load(f)
                    
                    # å¦‚æœæ£€æµ‹åˆ°æ˜¯ç®€åŒ–é…ç½®ï¼Œåº”ç”¨è‡ªåŠ¨ä¼˜åŒ–
                    if 'optimization' in config and config['optimization'].get('auto_optimize', False):
                        logger.info("[CONFIG] æ£€æµ‹åˆ°ç®€åŒ–é…ç½®ï¼Œå¼€å§‹è‡ªåŠ¨ä¼˜åŒ–...")
                        
                        # è·å–æ‰‹åŠ¨è¦†ç›–è®¾ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
                        manual_gpu_tier = config['optimization'].get('gpu_tier')
                        manual_gpu_memory = config['optimization'].get('gpu_memory_gb')
                        
                        # åº”ç”¨æ‰‹åŠ¨è¦†ç›–ï¼ˆå¦‚æœæœ‰ï¼‰
                        if manual_gpu_tier:
                            hardware_detector.gpu_info['gpu_tier'] = manual_gpu_tier
                            logger.info(f"[SETUP] æ‰‹åŠ¨è®¾ç½®GPUçº§åˆ«: {manual_gpu_tier}")
                        
                        if manual_gpu_memory:
                            hardware_detector.gpu_info['memory_total'] = manual_gpu_memory
                            logger.info(f"[SETUP] æ‰‹åŠ¨è®¾ç½®GPUæ˜¾å­˜: {manual_gpu_memory}GB")
                        
                        # ä¿å­˜ç”¨æˆ·åœ¨ [advanced] éƒ¨åˆ†è®¾ç½®çš„å€¼
                        user_advanced = config.get('advanced', {})
                        
                        # åº”ç”¨ä¼˜åŒ–é…ç½®
                        optimized_config = hardware_detector.get_optimized_config({})
                        
                        # æ›´æ–°argså¯¹è±¡ï¼ˆä½†ä¿ç•™ç”¨æˆ·æ˜¾å¼è®¾ç½®çš„å€¼ï¼‰
                        for key, value in optimized_config.items():
                            if hasattr(args, key):
                                # å¦‚æœç”¨æˆ·åœ¨ [advanced] ä¸­è®¾ç½®äº†è¯¥å€¼ï¼Œåˆ™ä½¿ç”¨ç”¨æˆ·çš„å€¼
                                if key in user_advanced:
                                    logger.info(f"   {key}: {user_advanced[key]} (ç”¨æˆ·è®¾ç½®)")
                                    setattr(args, key, user_advanced[key])
                                else:
                                    setattr(args, key, value)
                        
                        logger.info("[OK] è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–å®Œæˆ")
                
                except Exception as e:
                    logger.warning(f"[WARN] é…ç½®æ–‡ä»¶è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–: {e}")
                    # ä½¿ç”¨é»˜è®¤ä¼˜åŒ–é…ç½®
                    optimized_config = hardware_detector.get_optimized_config({})
                    for key, value in optimized_config.items():
                        if hasattr(args, key):
                            setattr(args, key, value)
                    logger.info("[OK] ä½¿ç”¨é»˜è®¤ç¡¬ä»¶ä¼˜åŒ–é…ç½®")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
    )
    
    # è·å–åˆ†å¸ƒå¼è®­ç»ƒä¿¡æ¯
    world_size = getattr(accelerator, 'num_processes', None)
    rank = getattr(accelerator, 'rank', None)
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_seed(args.seed)
    
    logger.info("="*60)
    logger.info("[START] å¯åŠ¨ AC-RF è®­ç»ƒ")
    logger.info("="*60)
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"Turbo æ­¥æ•°: {args.turbo_steps}")
    logger.info(f"LoRA rank: {args.network_dim}")
    
    # 1. åŠ è½½æ¨¡å‹
    logger.info("\n[LOAD] åŠ è½½ Transformer...")
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
    
    transformer = load_transformer(
        transformer_path=args.dit,
        device=accelerator.device,
        torch_dtype=weight_dtype,
    )
    # =========================================================================
    # Refactored Model Loaded - No Monkey Patch Needed
    # =========================================================================

    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()
        transformer.train()
        # NOTE: Freeze is done AFTER LoRA is applied (see below)
        logger.info("  [MEM] æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨ (Gradient Checkpointing Enabled)")
    else:
         # Legacy unfreeze
         pass

    # =========================================================================
    
    # 1.1 é…ç½®SDPA (Scaled Dot-Product Attention)
    logger.info("\n[INIT] é…ç½® SDPA æ³¨æ„åŠ›åç«¯...")
    logger.info(f"  æ³¨æ„åŠ›åç«¯: {args.attention_backend}")
    logger.info(f"  ä¼˜åŒ–çº§åˆ«: {args.sdpa_optimize_level}")
    logger.info(f"  å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›: {args.use_memory_efficient_attention}")
    logger.info(f"  æ³¨æ„åŠ›dropout: {args.attention_dropout}")
    
    # é…ç½®æ³¨æ„åŠ›åç«¯
    if hasattr(transformer, 'set_attention_backend'):
        try:
            if args.enable_flash_attention:
                # å¦‚æœå¯ç”¨äº†flash attentionï¼Œå°è¯•åˆ‡æ¢åç«¯
                if args.attention_backend == "sdpa":
                    # æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ
                    if torch.cuda.is_available():
                        gpu_name = torch.cuda.get_device_name(0).upper()
                        if "A100" in gpu_name or "H100" in gpu_name:
                            transformer.set_attention_backend("_flash_3")
                            logger.info("  [OK] ç¡¬ä»¶æ£€æµ‹ï¼šå·²å¯ç”¨ Flash Attention 3")
                        elif "RTX" in gpu_name or "4090" in gpu_name or "4080" in gpu_name:
                            transformer.set_attention_backend("flash")
                            logger.info("  [OK] ç¡¬ä»¶æ£€æµ‹ï¼šå·²å¯ç”¨ Flash Attention 2")
                        else:
                            logger.info("  [WARN] ç¡¬ä»¶ä¸æ”¯æŒFlash Attentionï¼Œä½¿ç”¨é»˜è®¤SDPA")
                    else:
                        logger.info("  [WARN] æœªæ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨é»˜è®¤SDPA")
                else:
                    transformer.set_attention_backend(args.attention_backend)
                    logger.info(f"  [OK] å·²è®¾ç½®æ³¨æ„åŠ›åç«¯ä¸º: {args.attention_backend}")
        except Exception as e:
            logger.warning(f"  [WARN] è®¾ç½®æ³¨æ„åŠ›åç«¯å¤±è´¥: {e}")
            logger.info("  [FALLBACK] ç»§ç»­ä½¿ç”¨é»˜è®¤SDPAå®ç°")
    
    # é…ç½®SDPAç¯å¢ƒå˜é‡
    if args.force_deterministic:
        os.environ['TORCH_DETERMINISTIC'] = '1'
        logger.info("  [LOCK] å·²å¯ç”¨ç¡®å®šæ€§è®¡ç®—")
    
    if args.sdpa_optimize_level == "memory_efficient":
        os.environ['TORCH_CUDA_MEMORY_POOL'] = 'memory_efficient'
        logger.info("  [MEM] å·²å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼")
    
    # åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨
    logger.info(f"\n[MEM] åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨...")
    if args.blocks_to_swap > 0:
        logger.info(f"  Blocks to swap: {args.blocks_to_swap}")
    memory_config = {
        'block_swap_enabled': args.block_swap_enabled or args.blocks_to_swap > 0,
        'blocks_to_swap': args.blocks_to_swap,
        'memory_block_size': args.block_swap_block_size,
        'cpu_swap_buffer_size': args.block_swap_cpu_buffer_size,
        'swap_threshold': args.block_swap_swap_threshold,
        'swap_frequency': args.memory_swap_frequency,
        'smart_prefetch': args.block_swap_prefetch_enabled,
        'swap_strategy': args.block_swap_swap_strategy,
        'compressed_swap': args.block_swap_compression_enabled,
        'checkpoint_optimization': 'basic' if args.gradient_checkpointing else 'none',
    }
    memory_optimizer = MemoryOptimizer(memory_config)
    memory_optimizer.start()
    logger.info(f"  [OK] å†…å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()
        logger.info("  [FALLBACK] å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        
    # åº”ç”¨å†…å­˜ä¼˜åŒ–åˆ°transformer
    if hasattr(transformer, 'apply_memory_optimization'):
        transformer.apply_memory_optimization(memory_optimizer)
        logger.info("  [INIT] å·²åº”ç”¨å†…å­˜ä¼˜åŒ–ç­–ç•¥")
        
    # 2. åˆ›å»º LoRA ç½‘ç»œ
    logger.info(f"\n[SETUP] åˆ›å»º LoRA ç½‘ç»œ (rank={args.network_dim})...")
    network = LoRANetwork(
        unet=transformer,
        lora_dim=args.network_dim,
        alpha=args.network_alpha,
        multiplier=1.0,
    )
    network.apply_to(transformer)
    
    # å…³é”®: å…ˆåº”ç”¨ LoRAï¼Œå†å†»ç»“åº•æ¨¡ (LoRA å‚æ•°ä¸ä¼šè¢«å†»ç»“)
    if args.gradient_checkpointing:
        transformer.requires_grad_(False)  # å†»ç»“åº•æ¨¡
        logger.info("  [FREEZE] åº•æ¨¡å·²å†»ç»“ (Base model frozen, LoRA trainable)")
    
    # åªè·å– LoRA å±‚çš„å‚æ•°ï¼Œä¸åŒ…æ‹¬åŸå§‹æ¨¡å‹
    trainable_params = []
    for lora_module in network.lora_modules.values():
        trainable_params.extend(lora_module.get_trainable_params())
    
    lora_param_count = sum(p.numel() for p in trainable_params)
    logger.info(f"LoRA å¯è®­ç»ƒå‚æ•°: {lora_param_count:,} ({lora_param_count/1e6:.2f}M)")
    
    # 3. åˆ›å»º AC-RF Trainer
    logger.info(f"\n[INIT] åˆå§‹åŒ– AC-RF Trainer...")
    acrf_trainer = ACRFTrainer(
        num_train_timesteps=1000,
        turbo_steps=args.turbo_steps,
        shift=args.shift,
    )
    acrf_trainer.verify_setup()
    
    # 3.5. æ‰“å° Min-SNR é…ç½®å’Œé”šç‚¹æƒé‡åˆ†å¸ƒ
    snr_gamma = getattr(args, 'snr_gamma', 5.0)
    snr_floor = getattr(args, 'snr_floor', 0.1)
    logger.info(f"\n[SNR] Min-SNR é…ç½®: gamma={snr_gamma}, floor={snr_floor}")
    if snr_gamma > 0:
        print_anchor_snr_weights(
            turbo_steps=args.turbo_steps,
            shift=args.shift,
            snr_gamma=snr_gamma,
            snr_floor=snr_floor,
        )
    
    # 3.6. åˆ›å»ºé«˜çº§æŸå¤±å‡½æ•° (åŸºäºå¼€å…³åˆ¤æ–­)
    logger.info(f"\n[LOSS] è‡ªç”±ç»„åˆæŸå¤±æ¨¡å¼")
    logger.info(f"  [åŸºç¡€] lambda_l1={args.lambda_l1}, lambda_cosine={args.lambda_cosine}")
    
    frequency_loss_fn = None
    style_loss_fn = None
    
    # é¢‘åŸŸæ„ŸçŸ¥æŸå¤± (å¼€å…³æ§åˆ¶)
    enable_freq = getattr(args, 'enable_freq', False)
    if enable_freq:
        logger.info(f"  [é¢‘åŸŸæ„ŸçŸ¥] âœ… å¯ç”¨ lambda={args.lambda_freq}, alpha_hf={args.alpha_hf}, beta_lf={args.beta_lf}")
        frequency_loss_fn = FrequencyAwareLoss(
            alpha_hf=args.alpha_hf,
            beta_lf=args.beta_lf,
            base_weight=1.0,
            downsample_factor=args.downsample_factor,
            lf_magnitude_weight=args.lf_magnitude_weight,
        )
    
    # é£æ ¼ç»“æ„æŸå¤± (å¼€å…³æ§åˆ¶)
    enable_style = getattr(args, 'enable_style', False)
    if enable_style:
        logger.info(f"  [é£æ ¼ç»“æ„] âœ… å¯ç”¨ lambda={args.lambda_style}, struct={args.lambda_struct}")
        style_loss_fn = LatentStyleStructureLoss(
            lambda_struct=args.lambda_struct,
            lambda_light=args.lambda_light,
            lambda_color=args.lambda_color,
            lambda_tex=args.lambda_tex,
            lambda_base=1.0,
        )
    
    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    logger.info("\nğŸ“Š åŠ è½½æ•°æ®é›†...")
    dataloader = create_dataloader(args)
    logger.info(f"æ•°æ®é›†å¤§å°: {len(dataloader)} batches")
    
    # 5. è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(dataloader) / args.gradient_accumulation_steps)
    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Num Batches per Epoch = {len(dataloader)}")
    logger.info(f"  Gradient Accumulation = {args.gradient_accumulation_steps}")
    logger.info(f"  Total Optimization Steps = {args.max_train_steps}")
    
    # æ‰“å°æ€»æ­¥æ•°ä¾›å‰ç«¯è§£æï¼ˆåªè®©ä¸»è¿›ç¨‹æ‰“å°ï¼‰
    if accelerator.is_main_process:
        print(f"[TRAINING_INFO] total_steps={args.max_train_steps} total_epochs={args.num_train_epochs}", flush=True)

    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    logger.info(f"\n[SETUP] åˆå§‹åŒ–ä¼˜åŒ–å™¨: {args.optimizer_type}")
    
    if args.optimizer_type == "AdamW":
        optimizer = torch.optim.AdamW(
            trainable_params, 
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
    elif args.optimizer_type == "AdamW8bit":
        try:
            import bitsandbytes as bnb
            optimizer = bnb.optim.AdamW8bit(
                trainable_params, 
                lr=args.learning_rate,
                weight_decay=args.weight_decay
            )
        except ImportError:
            raise ImportError("è¯·å…ˆå®‰è£… bitsandbytes ä»¥ä½¿ç”¨ AdamW8bit ä¼˜åŒ–å™¨")
    elif args.optimizer_type == "Adafactor":
        from transformers.optimization import Adafactor
        logger.info(f"  Adafactor é…ç½®: scale={args.adafactor_scale}, relative={args.adafactor_relative}, warmup={args.adafactor_warmup}")
        optimizer = Adafactor(
            trainable_params,
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            scale_parameter=args.adafactor_scale,
            relative_step=args.adafactor_relative,
            warmup_init=args.adafactor_warmup
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {args.optimizer_type}")
        
    # 7. åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    # æ³¨æ„ï¼šlr_scheduler.step() åªåœ¨ä¼˜åŒ–å™¨æ­¥éª¤æ—¶è°ƒç”¨ï¼ˆsync_gradients æ—¶ï¼‰
    # æ‰€ä»¥ num_warmup_steps å’Œ num_training_steps åº”è¯¥æ˜¯ä¼˜åŒ–å™¨æ­¥æ•°ï¼Œä¸éœ€è¦ä¹˜ä»¥æ¢¯åº¦ç´¯ç§¯
    from diffusers.optimization import get_scheduler
    logger.info(f"[SCHED] åˆå§‹åŒ–è°ƒåº¦å™¨: {args.lr_scheduler} (warmup={args.lr_warmup_steps}, total_steps={args.max_train_steps}, cycles={args.lr_num_cycles})")
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps,
        num_training_steps=args.max_train_steps,
        num_cycles=args.lr_num_cycles,
    )
    
    # 7. Accelerator prepare
    transformer, network, optimizer, dataloader, lr_scheduler = accelerator.prepare(
        transformer, network, optimizer, dataloader, lr_scheduler
    )
    
    # 8. è®­ç»ƒå¾ªç¯
    logger.info("\n" + "="*60)
    logger.info("[TARGET] å¼€å§‹è®­ç»ƒ")
    logger.info("="*60)
    
    global_step = 0
    # ç¦ç”¨ tqdm æ˜¾ç¤ºï¼Œæ”¹ç”¨ [STEP] æ ¼å¼è¾“å‡ºï¼ˆé¿å…æ—¥å¿—é‡å¤ï¼‰
    progress_bar = tqdm(total=args.max_train_steps, desc="Training", disable=True)
    
    # EMA å¹³æ»‘ lossï¼ˆç”¨äºæ˜¾ç¤ºè¶‹åŠ¿ï¼Œä¸å½±å“è®­ç»ƒï¼‰
    ema_loss = None
    ema_decay = 0.99  # å¹³æ»‘ç³»æ•°
    
    for epoch in range(args.num_train_epochs):
        logger.info(f"\nEpoch {epoch + 1}/{args.num_train_epochs}")
        transformer.train()
        
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(network):
                # è·å–æ•°æ®
                latents = batch['latents'].to(accelerator.device, dtype=weight_dtype)
                vl_embed = batch['vl_embed']  # List of tensors
                
                # ç¡®ä¿ vl_embed ä¸­çš„æ‰€æœ‰å¼ é‡éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if isinstance(vl_embed, list):
                    vl_embed = [tensor.to(accelerator.device, dtype=weight_dtype) for tensor in vl_embed]
                else:
                    vl_embed = vl_embed.to(accelerator.device, dtype=weight_dtype)
                
                # ç”Ÿæˆå™ªå£°
                noise = torch.randn_like(latents)
                
                # AC-RF é‡‡æ · (æ—¶é—´æ­¥ jitter)
                noisy_latents, timesteps, target_velocity = acrf_trainer.sample_batch(
                    latents, noise, jitter_scale=args.jitter_scale
                )
                
                # === Latent Jitter: ç©ºé—´æŠ åŠ¨ (å‚ç›´äºæµçº¿ï¼Œæ”¹å˜æ„å›¾çš„å…³é”®) ===
                latent_jitter_scale = getattr(args, 'latent_jitter_scale', 0.0)
                if latent_jitter_scale > 0:
                    # åœ¨ x_t ä¸Šæ·»åŠ ç©ºé—´æŠ–åŠ¨ï¼ŒæŠŠçŠ¶æ€â€œæ¨ç¦»â€å®Œç¾æµçº¿
                    latent_jitter = torch.randn_like(noisy_latents) * latent_jitter_scale
                    noisy_latents_perturbed = noisy_latents + latent_jitter
                    
                    # å…³é”®: é‡æ–°è®¡ç®— targetï¼ŒæŒ‡å‘çœŸå® x_0 (Ground Truth)
                    # v_target = x_0 - x_t_perturbed (ä¸æ˜¯ Teacher è¾“å‡º!)
                    target_velocity = noise - latents  # v = epsilon - x0 (RF å…¬å¼)
                    # ä½†è¾“å…¥æ˜¯æ‰°åŠ¨åçš„ x_t
                    noisy_latents = noisy_latents_perturbed
                
                # å‡†å¤‡æ¨¡å‹è¾“å…¥
                # Z-Image expects List[Tensor(C, 1, H, W)]
                model_input = noisy_latents.unsqueeze(2)  # (B, C, 1, H, W)
                
                # å…³é”®: æ¢¯åº¦æ£€æŸ¥ç‚¹éœ€è¦è¾“å…¥æœ‰æ¢¯åº¦ (ä¸ LongCat ç›¸åŒæ¨¡å¼)
                if args.gradient_checkpointing:
                    model_input.requires_grad_(True)
                    
                model_input_list = list(model_input.unbind(dim=0))
                
                # Timestep normalization (Z-Image uses (1000-t)/1000)
                timesteps_normalized = (1000 - timesteps) / 1000.0
                timesteps_normalized = timesteps_normalized.to(dtype=weight_dtype)
                
                # å‰å‘ä¼ æ’­
                model_pred_list = transformer(
                    x=model_input_list,
                    t=timesteps_normalized,
                    cap_feats=vl_embed,
                )[0]
                
                # Stack outputs
                model_pred = torch.stack(model_pred_list, dim=0)
                model_pred = model_pred.squeeze(2)  # (B, C, H, W)
                
                # Z-Image è¾“å‡ºæ˜¯è´Ÿçš„
                model_pred = -model_pred
                
                # æ ¹æ®æŸå¤±æ¨¡å¼è®¡ç®—æŸå¤±
                loss_components = {}
                
                # === Charbonnier Loss (Robust L1, åŸºç¡€æŸå¤±) ===
                diff = model_pred - target_velocity
                loss_l1 = torch.sqrt(diff**2 + 1e-6).mean()
                loss_components['l1'] = loss_l1.item()
                
                # === Cosine Loss (æ–¹å‘ä¸€è‡´æ€§) ===
                pred_flat = model_pred.view(model_pred.shape[0], -1)
                target_flat = target_velocity.view(target_velocity.shape[0], -1)
                cos_sim = F.cosine_similarity(pred_flat, target_flat, dim=1).mean()
                loss_cosine = 1.0 - cos_sim
                loss_components['cosine'] = loss_cosine.item()
                
                # è®¡ç®— Min-SNR æƒé‡ï¼ˆç»Ÿä¸€åº”ç”¨äºæ‰€æœ‰ loss æ¨¡å¼ï¼‰
                if snr_gamma > 0:
                    snr_weights = compute_snr_weights(
                        timesteps=timesteps,
                        num_train_timesteps=1000,
                        snr_floor=snr_floor,
                        prediction_type="v_prediction",
                    ).to(model_pred.device, dtype=weight_dtype)
                else:
                    snr_weights = None
                
                # === è‡ªç”±ç»„åˆæŸå¤± (æƒé‡æ§åˆ¶) ===
                # åŸºç¡€æŸå¤±: L1 + Cosine
                loss = args.lambda_l1 * loss_l1 + args.lambda_cosine * loss_cosine
                
                # å¯é€‰: é¢‘åŸŸæ„ŸçŸ¥æŸå¤±
                if enable_freq and frequency_loss_fn is not None:
                    freq_loss, freq_comps = frequency_loss_fn(
                        pred_v=model_pred,
                        target_v=target_velocity,
                        noisy_latents=noisy_latents,
                        timesteps=timesteps,
                        num_train_timesteps=1000,
                        return_components=True,
                    )
                    loss = loss + args.lambda_freq * freq_loss
                    loss_components['freq'] = freq_loss.item()
                
                # å¯é€‰: é£æ ¼ç»“æ„æŸå¤±
                if enable_style and style_loss_fn is not None:
                    style_loss, style_comps = style_loss_fn(
                        pred_v=model_pred,
                        target_v=target_velocity,
                        noisy_latents=noisy_latents,
                        timesteps=timesteps,
                        num_train_timesteps=1000,
                        return_components=True,
                    )
                    loss = loss + args.lambda_style * style_loss
                    loss_components['style'] = style_loss.item()
                
                # === RAFT: åŒ Batch æ··åˆæ¨¡å¼ (é”šç‚¹æµ + è‡ªç”±æµ) ===
                raft_mode = getattr(args, 'raft_mode', False)
                free_stream_ratio = getattr(args, 'free_stream_ratio', 0.3)
                lambda_mse = getattr(args, 'lambda_mse', 0.0)
                
                if raft_mode and free_stream_ratio > 0:
                    # RAFT æ¨¡å¼: åŒ batch å†…æ··åˆè®¡ç®—è‡ªç”±æµæŸå¤±
                    batch_size = latents.shape[0]
                    
                    # è‡ªç”±æµ: å…¨æ—¶é—´æ­¥éšæœºé‡‡æ ·
                    free_sigmas = torch.rand(batch_size, device=latents.device, dtype=latents.dtype)
                    shift = args.shift
                    free_sigmas = (free_sigmas * shift) / (1 + (shift - 1) * free_sigmas)
                    free_sigmas = free_sigmas.clamp(0.001, 0.999)
                    
                    # æ„é€ è‡ªç”±æµåŠ å™ª latents
                    sigma_broadcast = free_sigmas.view(batch_size, 1, 1, 1)
                    free_noisy = sigma_broadcast * noise + (1 - sigma_broadcast) * latents
                    free_target = noise - latents  # v-prediction
                    
                    # è‡ªç”±æµå‰å‘ä¼ æ’­ (å‚ä¸æ¢¯åº¦!)
                    free_input = free_noisy.unsqueeze(2)
                    free_input_list = list(free_input.unbind(dim=0))
                    free_t_norm = (1000 - free_sigmas * 1000) / 1000.0
                    free_t_norm = free_t_norm.to(dtype=weight_dtype)
                    
                    free_pred_list = transformer(
                        x=free_input_list,
                        t=free_t_norm,
                        cap_feats=vl_embed,
                    )[0]
                    free_pred = torch.stack(free_pred_list, dim=0).squeeze(2)
                    free_pred = -free_pred  # Z-Image è´Ÿå·
                    
                    # è‡ªç”±æµ L2 æŸå¤±
                    loss_free = F.mse_loss(free_pred, free_target)
                    
                    # RAFT æ··åˆ: loss_total = loss_anchor + ratio * loss_free
                    loss = loss + free_stream_ratio * loss_free
                    loss_components['loss_free'] = loss_free.item()
                
                elif lambda_mse > 0:
                    # å…¼å®¹æ—§ç‰ˆ: ç‹¬ç«‹ L2 æŸå¤± (ä¸å‚ä¸æ¢¯åº¦)
                    mse_use_anchor = getattr(args, 'mse_use_anchor', False)
                    if mse_use_anchor:
                        mse_pred = model_pred
                        mse_target = target_velocity
                    else:
                        batch_size = latents.shape[0]
                        mse_sigmas = torch.rand(batch_size, device=latents.device, dtype=latents.dtype)
                        shift = args.shift
                        mse_sigmas = (mse_sigmas * shift) / (1 + (shift - 1) * mse_sigmas)
                        mse_sigmas = mse_sigmas.clamp(0.001, 0.999)
                        
                        sigma_broadcast = mse_sigmas.view(batch_size, 1, 1, 1)
                        mse_noisy = sigma_broadcast * noise + (1 - sigma_broadcast) * latents
                        mse_target = noise - latents
                        
                        mse_input = mse_noisy.unsqueeze(2)
                        mse_input_list = list(mse_input.unbind(dim=0))
                        mse_t_norm = (1000 - mse_sigmas * 1000) / 1000.0
                        mse_t_norm = mse_t_norm.to(dtype=weight_dtype)
                        
                        with torch.no_grad():
                            mse_pred_list = transformer(
                                x=mse_input_list,
                                t=mse_t_norm,
                                cap_feats=vl_embed,
                            )[0]
                        mse_pred = torch.stack(mse_pred_list, dim=0).squeeze(2)
                        mse_pred = -mse_pred
                    
                    loss_mse = F.mse_loss(mse_pred, mse_target)
                    loss = loss + lambda_mse * loss_mse
                    loss_components['mse'] = loss_mse.item()
                
                # åº”ç”¨ SNR åŠ æƒ
                if snr_weights is not None:
                    loss = loss * snr_weights.mean()
                
                # å¼ºåˆ¶è½¬æ¢ä¸º Float32 ä»¥å…¼å®¹ Accelerate çš„ backward (BF16 æ··åˆç²¾åº¦ä¿®å¤)
                loss = loss.float()
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
            
            # åªåœ¨æ¢¯åº¦ç´¯ç§¯å®Œæˆåæ‰§è¡Œä¼˜åŒ–æ­¥éª¤
            if accelerator.sync_gradients:
                # æ¢¯åº¦è£å‰ª
                accelerator.clip_grad_norm_(trainable_params, args.max_grad_norm)
                
                # ä¼˜åŒ–å™¨æ­¥è¿›
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
                
                # Cleanup transformer gradients (since we unfroze it but don't optimize it)
                transformer.zero_grad()
                
                # æ›´æ–°è¿›åº¦
                progress_bar.update(1)
                global_step += 1
                
                # æ›´æ–° EMA lossï¼ˆå¹³æ»‘æ˜¾ç¤ºï¼Œå‡å°‘è·³åŠ¨çš„è§†è§‰å¹²æ‰°ï¼‰
                current_loss = loss.item()
                if ema_loss is None:
                    ema_loss = current_loss
                else:
                    ema_loss = ema_decay * ema_loss + (1 - ema_decay) * current_loss
                
                # è·å–å½“å‰å­¦ä¹ ç‡
                current_lr = lr_scheduler.get_last_lr()[0]
                
                # æ‰“å°è¿›åº¦ä¾›å‰ç«¯è§£æï¼ˆåªè®©ä¸»è¿›ç¨‹æ‰“å°ï¼‰
                if accelerator.is_main_process:
                    l1 = loss_components.get('l1', 0)
                    cosine = loss_components.get('cosine', 0)
                    freq = loss_components.get('freq', 0)
                    style = loss_components.get('style', 0)
                    free = loss_components.get('loss_free', 0)
                    print(f"[STEP] {global_step}/{args.max_train_steps} epoch={epoch+1}/{args.num_train_epochs} loss={current_loss:.4f} ema={ema_loss:.4f} l1={l1:.4f} cos={cosine:.4f} freq={freq:.4f} style={style:.4f} free={free:.4f} lr={current_lr:.2e}", flush=True)
                
            # æ‰§è¡Œå†…å­˜ä¼˜åŒ– (æ¸…ç†ç¼“å­˜ç­‰)
            memory_optimizer.optimize_training_step()
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if _interrupted:
                logger.info(f"\n[STOP] ä¸­æ–­è®­ç»ƒï¼Œä¿å­˜å½“å‰è¿›åº¦...")
                interrupt_path = Path(args.output_dir) / f"{args.output_name}_interrupted_step{global_step}.safetensors"
                network.save_weights(interrupt_path, dtype=weight_dtype)
                logger.info(f"[SAVE] å·²ä¿å­˜ä¸­æ–­æ£€æŸ¥ç‚¹: {interrupt_path}")
                memory_optimizer.stop()
                logger.info("[EXIT] 5ç§’åé€€å‡ºè¿›ç¨‹...")
                time.sleep(5)
                os._exit(0)
                
        # Epoch ç»“æŸï¼Œä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_every_n_epochs == 0:
            save_path = Path(args.output_dir) / f"{args.output_name}_epoch{epoch+1}.safetensors"
            network.save_weights(save_path, dtype=weight_dtype)
            logger.info(f"\n[SAVE] ä¿å­˜æ£€æŸ¥ç‚¹ (Epoch {epoch+1}): {save_path}")
            
            # æ˜¾å¼æ¸…ç†æ˜¾å­˜ (é˜²æ­¢ 16G æ˜¾å¡æ˜¾å­˜æ³„éœ²)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = Path(args.output_dir) / f"{args.output_name}_final.safetensors"
    network.save_weights(final_path, dtype=weight_dtype)
    
    # åœæ­¢å†…å­˜ä¼˜åŒ–å™¨å¹¶æ¸…ç†æ˜¾å­˜
    memory_optimizer.stop()
    
    # æ¸…ç† GPU ç¼“å­˜
    del network, transformer, optimizer, lr_scheduler, dataloader
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
    
    logger.info("\n" + "="*60)
    logger.info(f"[OK] è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æœ€ç»ˆæ¨¡å‹: {final_path}")
    logger.info("="*60)
    
    # 5ç§’åå¼ºåˆ¶é€€å‡ºè¿›ç¨‹ï¼Œç¡®ä¿æ˜¾å­˜é‡Šæ”¾
    logger.info("\n[EXIT] 5ç§’åé€€å‡ºè¿›ç¨‹...")
    time.sleep(5)
    os._exit(0)


if __name__ == "__main__":
    main()
