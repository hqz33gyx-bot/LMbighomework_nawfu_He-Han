"""
[START] Frequency-Aware Training Script for Z-Image-Turbo

é¢‘åŸŸæ„ŸçŸ¥è®­ç»ƒè„šæœ¬ - åŸºäºé¢‘åŸŸåˆ†ç¦»çš„è§£è€¦å­¦ä¹ ç­–ç•¥

æ ¸å¿ƒç­–ç•¥ï¼š
- é«˜é¢‘å¢å¼ºï¼šL1 Loss å¼ºåŒ–çº¹ç†/è¾¹ç¼˜ç»†èŠ‚ï¼Œè®©ç”»é¢æ›´é”åˆ©
- ä½é¢‘é”å®šï¼šCosine Loss é”å®šç»“æ„/å…‰å½±æ–¹å‘ï¼Œé˜²æ­¢è‰²åå’Œé£æ ¼æ¼‚ç§»

é€‚ç”¨åœºæ™¯ï¼š
- æƒ³è¦æå‡ç»†èŠ‚æ¸…æ™°åº¦ä½†ä¸æ”¹å˜æ•´ä½“é£æ ¼
- å¾®è°ƒæ—¶å®¹æ˜“"é¡¾æ­¤å¤±å½¼"çš„æƒ…å†µ
- éœ€è¦ç²¾ç»†æ§åˆ¶é«˜é¢‘/ä½é¢‘å­¦ä¹ ç¨‹åº¦

Usage:
    python scripts/train_frequency_aware.py --config config/freq_aware_config.toml
"""

import os
import sys
import math
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../src"))

import torch
import argparse
from pathlib import Path
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed

from zimage_trainer.acrf_trainer import ACRFTrainer
from zimage_trainer.utils.zimage_utils import load_transformer
from zimage_trainer.networks.lora import LoRANetwork
from zimage_trainer.dataset.dataloader import create_dataloader
from zimage_trainer.utils.memory_optimizer import MemoryOptimizer
from zimage_trainer.utils.hardware_detector import HardwareDetector
from zimage_trainer.losses import FrequencyAwareLoss, AdaptiveFrequencyLoss

import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(description="Frequency-Aware è®­ç»ƒè„šæœ¬")
    
    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument("--config", type=str, help="è¶…å‚æ•°é…ç½®æ–‡ä»¶è·¯å¾„ (.toml)")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--dit", type=str, help="Transformer æ¨¡å‹è·¯å¾„")
    parser.add_argument("--dataset_config", type=str, help="æ•°æ®é›†é…ç½®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="output/freq_aware", help="è¾“å‡ºç›®å½•")
    
    # AC-RF å‚æ•°
    parser.add_argument("--turbo_steps", type=int, default=10, help="Turbo æ­¥æ•°ï¼ˆé”šç‚¹æ•°é‡ï¼‰")
    parser.add_argument("--shift", type=float, default=3.0, help="æ—¶é—´æ­¥ shift å‚æ•°")
    parser.add_argument("--jitter_scale", type=float, default=0.02, help="é”šç‚¹æŠ–åŠ¨å¹…åº¦")
    
    # LoRA å‚æ•°
    parser.add_argument("--network_dim", type=int, default=8, help="LoRA rank")
    parser.add_argument("--network_alpha", type=float, default=4.0, help="LoRA alpha")
    
    # é¢‘åŸŸæ„ŸçŸ¥ Loss å‚æ•°
    parser.add_argument("--alpha_hf", type=float, default=1.0, 
                       help="é«˜é¢‘å¢å¼ºæƒé‡ (æ¨è 0.5~1.0)")
    parser.add_argument("--beta_lf", type=float, default=0.2, 
                       help="ä½é¢‘é”å®šæƒé‡ (æ¨è 0.1~0.2)")
    parser.add_argument("--base_weight", type=float, default=1.0, 
                       help="åŸºç¡€ Loss æƒé‡")
    parser.add_argument("--downsample_factor", type=int, default=4, 
                       help="ä½é¢‘æå–é™é‡‡æ ·å› å­")
    parser.add_argument("--lf_magnitude_weight", type=float, default=0.0, 
                       help="ä½é¢‘å¹…åº¦çº¦æŸæƒé‡ (é˜²æ­¢å‘ç°ï¼Œå»ºè®® 0~0.1)")
    parser.add_argument("--adaptive_loss", action="store_true", 
                       help="å¯ç”¨è‡ªé€‚åº”é¢‘åŸŸ Lossï¼ˆåŠ¨æ€è°ƒæ•´æƒé‡ï¼‰")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--optimizer_type", type=str, default="AdamW", 
                       choices=["AdamW", "AdamW8bit", "Adafactor"])
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    
    # LR Scheduler å‚æ•°
    parser.add_argument("--lr_scheduler", type=str, default="constant", 
        choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"])
    parser.add_argument("--lr_warmup_steps", type=int, default=0, help="Warmup æ­¥æ•°")
    parser.add_argument("--lr_num_cycles", type=int, default=1, help="Cosine è°ƒåº¦å™¨çš„å¾ªç¯æ¬¡æ•°")
    
    # Min-SNR åŠ æƒï¼ˆä¸é¢‘åŸŸ Loss é…åˆä½¿ç”¨ï¼‰
    parser.add_argument("--snr_gamma", type=float, default=5.0, help="Min-SNR gamma (0=ç¦ç”¨)")
    
    # è®­ç»ƒæ§åˆ¶
    parser.add_argument("--num_train_epochs", type=int, default=10, help="è®­ç»ƒ Epoch æ•°")
    parser.add_argument("--save_every_n_epochs", type=int, default=1, help="ä¿å­˜é—´éš” (Epoch)")
    parser.add_argument("--output_name", type=str, default="zimage-freq-lora", help="è¾“å‡ºæ–‡ä»¶å")
    
    # é€šç”¨å‚æ•°
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--mixed_precision", type=str, default="bf16", choices=["no", "fp16", "bf16"])
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    parser.add_argument("--gradient_checkpointing", action="store_true")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œè¯»å–å¹¶è¦†ç›–é»˜è®¤å€¼
    if args.config:
        try:
            import tomli
        except ImportError:
            import tomllib as tomli
            
        with open(args.config, "rb") as f:
            config = tomli.load(f)
        
        defaults = {}
        for section in config.values():
            if isinstance(section, dict):
                defaults.update(section)
            
        parser.set_defaults(**defaults)
        args = parser.parse_args()
        
    if not args.dit:
        parser.error("--dit is required")
    
    if not args.dataset_config and args.config:
        args.dataset_config = args.config
        
    return args


def main():
    args = parse_args()
    
    # ç¡¬ä»¶æ£€æµ‹
    logger.info("[DETECT] æ­£åœ¨è¿›è¡Œç¡¬ä»¶æ£€æµ‹...")
    hardware_detector = HardwareDetector()
    hardware_detector.print_detection_summary()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
    )
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_seed(args.seed)
    
    logger.info("="*60)
    logger.info("[START] å¯åŠ¨ Frequency-Aware è®­ç»ƒ")
    logger.info("="*60)
    logger.info(f"ğŸ¨ è®­ç»ƒç­–ç•¥: é¢‘åŸŸåˆ†ç¦»è§£è€¦å­¦ä¹ ")
    logger.info(f"   é«˜é¢‘å¢å¼ºæƒé‡ (alpha_hf): {args.alpha_hf}")
    logger.info(f"   ä½é¢‘é”å®šæƒé‡ (beta_lf): {args.beta_lf}")
    logger.info(f"   é™é‡‡æ ·å› å­: {args.downsample_factor}")
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"LoRA rank: {args.network_dim}")
    
    # 1. åŠ è½½æ¨¡å‹
    logger.info("\n[LOAD] åŠ è½½ Transformer...")
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
    
    transformer = load_transformer(
        transformer_path=args.dit,
        device=accelerator.device,
        torch_dtype=weight_dtype,
    )
    transformer.requires_grad_(False)
    transformer.train()
    
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()
        logger.info("  [OK] æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    # 2. åˆ›å»º LoRA ç½‘ç»œ
    logger.info(f"\n[SETUP] åˆ›å»º LoRA ç½‘ç»œ (rank={args.network_dim})...")
    network = LoRANetwork(
        unet=transformer,
        lora_dim=args.network_dim,
        alpha=args.network_alpha,
        multiplier=1.0,
    )
    network.apply_to(transformer)
    
    trainable_params = []
    for lora_module in network.lora_modules.values():
        trainable_params.extend(lora_module.get_trainable_params())
    
    lora_param_count = sum(p.numel() for p in trainable_params)
    logger.info(f"LoRA å¯è®­ç»ƒå‚æ•°: {lora_param_count:,} ({lora_param_count/1e6:.2f}M)")
    
    # 3. åˆ›å»º AC-RF Trainerï¼ˆç”¨äºé‡‡æ ·ï¼‰
    logger.info(f"\n[INIT] åˆå§‹åŒ– AC-RF Trainer...")
    acrf_trainer = ACRFTrainer(
        num_train_timesteps=1000,
        turbo_steps=args.turbo_steps,
        shift=args.shift,
    )
    acrf_trainer.verify_setup()
    
    # 4. åˆ›å»ºé¢‘åŸŸæ„ŸçŸ¥ Loss
    logger.info(f"\n[LOSS] åˆå§‹åŒ– Frequency-Aware Loss...")
    if args.adaptive_loss:
        loss_fn = AdaptiveFrequencyLoss(
            alpha_hf=args.alpha_hf,
            beta_lf=args.beta_lf,
            base_weight=args.base_weight,
            downsample_factor=args.downsample_factor,
            lf_magnitude_weight=args.lf_magnitude_weight,
        )
        logger.info("  [OK] ä½¿ç”¨è‡ªé€‚åº”é¢‘åŸŸ Lossï¼ˆåŠ¨æ€æƒé‡ï¼‰")
    else:
        loss_fn = FrequencyAwareLoss(
            alpha_hf=args.alpha_hf,
            beta_lf=args.beta_lf,
            base_weight=args.base_weight,
            downsample_factor=args.downsample_factor,
            lf_magnitude_weight=args.lf_magnitude_weight,
        )
        logger.info("  [OK] ä½¿ç”¨å›ºå®šæƒé‡é¢‘åŸŸ Loss")
    
    # 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    logger.info("\n[DATA] åŠ è½½æ•°æ®é›†...")
    dataloader = create_dataloader(args)
    logger.info(f"æ•°æ®é›†å¤§å°: {len(dataloader)} batches")
    
    # 6. è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(dataloader) / args.gradient_accumulation_steps)
    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Total Optimization Steps = {args.max_train_steps}")
    
    if accelerator.is_main_process:
        print(f"[TRAINING_INFO] total_steps={args.max_train_steps} total_epochs={args.num_train_epochs}", flush=True)

    # 7. åˆ›å»ºä¼˜åŒ–å™¨
    logger.info(f"\n[SETUP] åˆå§‹åŒ–ä¼˜åŒ–å™¨: {args.optimizer_type}")
    
    if args.optimizer_type == "AdamW":
        optimizer = torch.optim.AdamW(
            trainable_params, 
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
    elif args.optimizer_type == "AdamW8bit":
        try:
            import bitsandbytes as bnb
            optimizer = bnb.optim.AdamW8bit(
                trainable_params, 
                lr=args.learning_rate,
                weight_decay=args.weight_decay
            )
        except ImportError:
            optimizer = torch.optim.AdamW(trainable_params, lr=args.learning_rate, weight_decay=args.weight_decay)
    elif args.optimizer_type == "Adafactor":
        from transformers.optimization import Adafactor
        optimizer = Adafactor(trainable_params, lr=args.learning_rate, weight_decay=args.weight_decay)
        
    # 8. åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    from diffusers.optimization import get_scheduler
    logger.info(f"[SCHED] åˆå§‹åŒ–è°ƒåº¦å™¨: {args.lr_scheduler} (warmup={args.lr_warmup_steps})")
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps,
        num_training_steps=args.max_train_steps,
        num_cycles=args.lr_num_cycles,
    )
    
    # 9. Accelerator prepare
    transformer, network, optimizer, dataloader, lr_scheduler = accelerator.prepare(
        transformer, network, optimizer, dataloader, lr_scheduler
    )
    
    # 10. å†…å­˜ä¼˜åŒ–å™¨
    memory_optimizer = MemoryOptimizer({'block_swap_enabled': False})
    memory_optimizer.start()
    
    # 11. è®­ç»ƒå¾ªç¯
    logger.info("\n" + "="*60)
    logger.info("[TARGET] å¼€å§‹é¢‘åŸŸæ„ŸçŸ¥è®­ç»ƒ")
    logger.info("="*60)
    
    global_step = 0
    progress_bar = tqdm(total=args.max_train_steps, desc="Freq-Aware Training", disable=True)
    
    # EMA å¹³æ»‘ loss
    ema_loss = None
    ema_decay = 0.99
    
    for epoch in range(args.num_train_epochs):
        logger.info(f"\nEpoch {epoch + 1}/{args.num_train_epochs}")
        transformer.train()
        
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(network):
                # è·å–æ•°æ®
                latents = batch['latents'].to(accelerator.device, dtype=weight_dtype)
                vl_embed = batch['vl_embed']
                
                if isinstance(vl_embed, list):
                    vl_embed = [tensor.to(accelerator.device, dtype=weight_dtype) for tensor in vl_embed]
                else:
                    vl_embed = vl_embed.to(accelerator.device, dtype=weight_dtype)
                
                # ç”Ÿæˆå™ªå£°
                noise = torch.randn_like(latents)
                
                # AC-RF é‡‡æ ·
                noisy_latents, timesteps, target_velocity = acrf_trainer.sample_batch(
                    latents, noise, jitter_scale=args.jitter_scale
                )
                
                # å‡†å¤‡æ¨¡å‹è¾“å…¥
                model_input = noisy_latents.unsqueeze(2)
                model_input_list = list(model_input.unbind(dim=0))
                
                # Timestep normalization
                timesteps_normalized = (1000 - timesteps) / 1000.0
                timesteps_normalized = timesteps_normalized.to(dtype=weight_dtype)
                
                # å‰å‘ä¼ æ’­
                model_pred_list = transformer(
                    x=model_input_list,
                    t=timesteps_normalized,
                    cap_feats=vl_embed,
                )[0]
                
                model_pred = torch.stack(model_pred_list, dim=0)
                model_pred = model_pred.squeeze(2)
                model_pred = -model_pred  # Z-Image è¾“å‡ºå–è´Ÿ
                
                # è®¡ç®—é¢‘åŸŸæ„ŸçŸ¥ Loss
                if args.adaptive_loss:
                    loss_fn.update_step(global_step)
                
                loss, loss_components = loss_fn(
                    pred_v=model_pred,
                    target_v=target_velocity,
                    noisy_latents=noisy_latents,
                    timesteps=timesteps,
                    num_train_timesteps=1000,
                    return_components=True,
                )
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
            
            # æ¢¯åº¦ç´¯ç§¯å®Œæˆåæ‰§è¡Œä¼˜åŒ–æ­¥éª¤
            if accelerator.sync_gradients:
                accelerator.clip_grad_norm_(trainable_params, args.max_grad_norm)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
                
                progress_bar.update(1)
                global_step += 1
                
                # æ›´æ–° EMA loss
                current_loss = loss.item()
                if ema_loss is None:
                    ema_loss = current_loss
                else:
                    ema_loss = ema_decay * ema_loss + (1 - ema_decay) * current_loss
                
                current_lr = lr_scheduler.get_last_lr()[0]
                
                # æ‰“å°è¿›åº¦ï¼ˆåªè®©ä¸»è¿›ç¨‹æ‰“å°ï¼‰
                if accelerator.is_main_process:
                    base_l = loss_components["base_loss"].item()
                    hf_l = loss_components["loss_hf"].item()
                    lf_l = loss_components["loss_lf"].item()
                    
                    print(f"[STEP] {global_step}/{args.max_train_steps} epoch={epoch+1}/{args.num_train_epochs} "
                          f"loss={current_loss:.4f} ema={ema_loss:.4f} base={base_l:.4f} hf={hf_l:.4f} lf={lf_l:.4f} "
                          f"lr={current_lr:.2e}", flush=True)
            
            memory_optimizer.optimize_training_step()
                
        # Epoch ç»“æŸï¼Œä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_every_n_epochs == 0:
            save_path = Path(args.output_dir) / f"{args.output_name}_epoch{epoch+1}.safetensors"
            network.save_weights(save_path, dtype=weight_dtype)
            logger.info(f"\n[SAVE] ä¿å­˜æ£€æŸ¥ç‚¹ (Epoch {epoch+1}): {save_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = Path(args.output_dir) / f"{args.output_name}_final.safetensors"
    network.save_weights(final_path, dtype=weight_dtype)
    
    memory_optimizer.stop()
    
    logger.info("\n" + "="*60)
    logger.info(f"[OK] é¢‘åŸŸæ„ŸçŸ¥è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æœ€ç»ˆæ¨¡å‹: {final_path}")
    logger.info("="*60)


if __name__ == "__main__":
    main()

