# -*- coding: utf-8 -*-
"""
Z-Image Finetune Inference Gradio UI

æ”¯æŒåŠ è½½ Full Finetune è®­ç»ƒçš„ 3.7G+ æ¨¡å‹æƒé‡ï¼Œ
å¹¶å¯å®æ—¶è°ƒèŠ‚æƒé‡æ··åˆæ¯”ä¾‹ï¼ˆåŸºç¡€æ¨¡å‹ vs å¾®è°ƒæ¨¡å‹ï¼‰ã€‚

Usage:
    python scripts/gradio_finetune_inference.py
"""

import os
import sys
import copy
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

import torch
import gradio as gr
from PIL import Image
from safetensors.torch import load_file
from diffusers import FlowMatchEulerDiscreteScheduler, AutoencoderKL

# --- å…¨å±€é…ç½® ---
DEFAULT_CONFIG = {
    "base_model": str(PROJECT_ROOT / "models" / "Z-Image-Turbo"),
    "vae_path": str(PROJECT_ROOT / "models" / "vae"),
    "text_encoder_path": str(PROJECT_ROOT / "models" / "qwen2_5_vl_3b"),
    "finetune_weights": "",  # ç”¨æˆ·é€‰æ‹©çš„ 3.7G æ¨¡å‹
}

# å…¨å±€çŠ¶æ€
_pipeline = None
_base_state_dict = None  # åŸºç¡€æ¨¡å‹æƒé‡ç¼“å­˜
_finetune_state_dict = None  # å¾®è°ƒæƒé‡ç¼“å­˜
_current_blend_ratio = 0.0


def get_device():
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def get_dtype():
    device = get_device()
    if device == "cuda" and torch.cuda.is_bf16_supported():
        return torch.bfloat16
    return torch.float32


def load_pipeline(base_model_path: str, vae_path: str, text_encoder_path: str):
    """åŠ è½½åŸºç¡€ Pipeline"""
    global _pipeline, _base_state_dict
    
    device = get_device()
    dtype = get_dtype()
    
    print(f"[INFO] Loading pipeline on {device} with dtype {dtype}")
    print(f"[INFO] Base model: {base_model_path}")
    
    # å°è¯•ä½¿ç”¨ diffusers åŸç”ŸåŠ è½½
    try:
        from diffusers import ZImagePipeline
        
        pipe = ZImagePipeline.from_pretrained(
            base_model_path,
            torch_dtype=dtype,
            local_files_only=True,
        )
    except Exception as e:
        print(f"[WARN] diffusers native load failed: {e}")
        print("[INFO] Attempting manual component loading...")
        
        # æ‰‹åŠ¨åŠ è½½ç»„ä»¶
        from zimage_trainer.utils.zimage_utils import (
            load_transformer,
            load_text_encoder_and_tokenizer,
            load_scheduler,
        )
        from zimage_trainer.utils.vae_utils import load_vae
        from zimage_trainer.z_image.pipeline_z_image import ZImagePipeline
        
        transformer = load_transformer(base_model_path, device=device, torch_dtype=dtype)
        vae = load_vae(vae_path, device=device, dtype=dtype)
        text_encoder, tokenizer = load_text_encoder_and_tokenizer(text_encoder_path, device=device)
        scheduler = load_scheduler("flow_match_euler", use_diffusers=True)
        
        pipe = ZImagePipeline(
            scheduler=scheduler,
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            transformer=transformer,
        )
    
    # ç¼“å­˜åŸºç¡€æ¨¡å‹æƒé‡
    _base_state_dict = {}
    for name, param in pipe.transformer.named_parameters():
        _base_state_dict[name] = param.data.clone().cpu()
    
    print(f"[INFO] Cached {len(_base_state_dict)} base model tensors")
    
    pipe.to(device)
    _pipeline = pipe
    
    return pipe


def load_finetune_weights(finetune_path: str):
    """åŠ è½½å¾®è°ƒæƒé‡"""
    global _finetune_state_dict
    
    if not finetune_path or not Path(finetune_path).exists():
        return None, "å¾®è°ƒæƒé‡æ–‡ä»¶ä¸å­˜åœ¨"
    
    print(f"[INFO] Loading finetune weights: {finetune_path}")
    _finetune_state_dict = load_file(finetune_path)
    
    size_mb = sum(t.numel() * t.element_size() for t in _finetune_state_dict.values()) / 1024 / 1024
    print(f"[INFO] Loaded {len(_finetune_state_dict)} finetune tensors ({size_mb:.1f} MB)")
    
    return _finetune_state_dict, f"æˆåŠŸåŠ è½½ {len(_finetune_state_dict)} ä¸ªå¼ é‡ ({size_mb:.1f} MB)"


def blend_weights(blend_ratio: float):
    """æ··åˆåŸºç¡€æƒé‡å’Œå¾®è°ƒæƒé‡
    
    blend_ratio: 0.0 = çº¯åŸºç¡€æ¨¡å‹, 1.0 = çº¯å¾®è°ƒæ¨¡å‹
    """
    global _pipeline, _base_state_dict, _finetune_state_dict, _current_blend_ratio
    
    if _pipeline is None:
        return "è¯·å…ˆåŠ è½½åŸºç¡€æ¨¡å‹"
    
    if _finetune_state_dict is None:
        return "è¯·å…ˆåŠ è½½å¾®è°ƒæƒé‡"
    
    _current_blend_ratio = blend_ratio
    device = get_device()
    dtype = get_dtype()
    
    print(f"[INFO] Blending weights: base={1-blend_ratio:.2f}, finetune={blend_ratio:.2f}")
    
    blended_count = 0
    with torch.no_grad():
        for name, param in _pipeline.transformer.named_parameters():
            if name in _finetune_state_dict and name in _base_state_dict:
                base_weight = _base_state_dict[name].to(device=device, dtype=dtype)
                finetune_weight = _finetune_state_dict[name].to(device=device, dtype=dtype)
                
                # çº¿æ€§æ’å€¼æ··åˆ
                blended = (1 - blend_ratio) * base_weight + blend_ratio * finetune_weight
                param.data.copy_(blended)
                blended_count += 1
    
    return f"å·²æ··åˆ {blended_count} ä¸ªå¼ é‡ (åŸºç¡€:{1-blend_ratio:.0%} + å¾®è°ƒ:{blend_ratio:.0%})"


def generate_image(
    prompt: str,
    negative_prompt: str,
    width: int,
    height: int,
    steps: int,
    guidance_scale: float,
    seed: int,
    blend_ratio: float,
    shift: float = 3.0,
):
    """ç”Ÿæˆå›¾åƒ"""
    global _pipeline, _current_blend_ratio
    
    if _pipeline is None:
        return None, "è¯·å…ˆåŠ è½½æ¨¡å‹"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ··åˆæƒé‡
    if _finetune_state_dict is not None and abs(blend_ratio - _current_blend_ratio) > 0.001:
        blend_weights(blend_ratio)
    
    # è®¾ç½®éšæœºç§å­
    device = get_device()
    if seed == -1:
        generator = torch.Generator(device=device)
        actual_seed = generator.seed()
    else:
        generator = torch.Generator(device=device).manual_seed(seed)
        actual_seed = seed
    
    print(f"[INFO] Generating: {width}x{height}, steps={steps}, cfg={guidance_scale}, seed={actual_seed}, shift={shift}")
    print(f"[INFO] Prompt: {prompt[:100]}...")
    
    # åº”ç”¨ shift å‚æ•°åˆ° scheduler
    if shift > 0:
        _pipeline.scheduler.config["base_shift"] = shift
        _pipeline.scheduler.config["max_shift"] = shift
    
    try:
        image = _pipeline(
            prompt=prompt,
            negative_prompt=negative_prompt if negative_prompt else None,
            width=width,
            height=height,
            num_inference_steps=steps,
            guidance_scale=guidance_scale,
            generator=generator,
        ).images[0]
        
        return image, f"ç”ŸæˆæˆåŠŸï¼Seed: {actual_seed}"
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, f"ç”Ÿæˆå¤±è´¥: {str(e)}"


def scan_finetune_models(directory: str):
    """æ‰«æç›®å½•ä¸‹çš„å¾®è°ƒæ¨¡å‹æ–‡ä»¶"""
    models = []
    
    if not directory:
        directory = str(PROJECT_ROOT / "output")
    
    path = Path(directory)
    if not path.exists():
        return models
    
    for f in path.rglob("*.safetensors"):
        size_mb = f.stat().st_size / 1024 / 1024
        # åªæ˜¾ç¤ºå¤§äº 100MB çš„æ–‡ä»¶ï¼ˆæ’é™¤ LoRAï¼‰
        if size_mb > 100:
            models.append({
                "path": str(f),
                "name": f.name,
                "size": f"{size_mb:.1f} MB"
            })
    
    return sorted(models, key=lambda x: x["name"])


# ============================================================
# Gradio UI
# ============================================================

def create_ui():
    with gr.Blocks(title="Z-Image Finetune Inference") as demo:
        gr.Markdown("# ğŸ¨ Z-Image Finetune æ¨ç†å·¥å…·")
        gr.Markdown("æ”¯æŒåŠ è½½ Full Finetune è®­ç»ƒçš„æ¨¡å‹æƒé‡ï¼Œå¯å®æ—¶è°ƒèŠ‚åŸºç¡€æ¨¡å‹ä¸å¾®è°ƒæ¨¡å‹çš„æ··åˆæ¯”ä¾‹ã€‚")
        
        with gr.Row():
            # å·¦ä¾§ï¼šè®¾ç½®é¢æ¿
            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ æ¨¡å‹è®¾ç½®")
                
                base_model_input = gr.Textbox(
                    label="åŸºç¡€æ¨¡å‹è·¯å¾„",
                    value=DEFAULT_CONFIG["base_model"],
                    placeholder="Z-Image-Turbo å®Œæ•´æ¨¡å‹ç›®å½•",
                )
                
                vae_input = gr.Textbox(
                    label="VAE è·¯å¾„",
                    value=DEFAULT_CONFIG["vae_path"],
                    placeholder="å¯é€‰ï¼Œç•™ç©ºä½¿ç”¨æ¨¡å‹å†…ç½® VAE",
                )
                
                text_encoder_input = gr.Textbox(
                    label="Text Encoder è·¯å¾„",
                    value=DEFAULT_CONFIG["text_encoder_path"],
                    placeholder="å¯é€‰ï¼Œç•™ç©ºä½¿ç”¨æ¨¡å‹å†…ç½® TE",
                )
                
                load_base_btn = gr.Button("ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹", variant="primary")
                base_status = gr.Textbox(label="çŠ¶æ€", interactive=False)
                
                gr.Markdown("---")
                gr.Markdown("### ğŸ”§ å¾®è°ƒæƒé‡")
                
                finetune_dir = gr.Textbox(
                    label="å¾®è°ƒæ¨¡å‹ç›®å½•",
                    value=str(PROJECT_ROOT / "output"),
                    placeholder="æ‰«æè¯¥ç›®å½•ä¸‹çš„ .safetensors æ–‡ä»¶",
                )
                
                scan_btn = gr.Button("ğŸ” æ‰«ææ¨¡å‹")
                
                finetune_dropdown = gr.Dropdown(
                    label="é€‰æ‹©å¾®è°ƒæ¨¡å‹",
                    choices=[],
                    interactive=True,
                )
                
                load_finetune_btn = gr.Button("ğŸ“‚ åŠ è½½å¾®è°ƒæƒé‡")
                finetune_status = gr.Textbox(label="å¾®è°ƒçŠ¶æ€", interactive=False)
                
                gr.Markdown("---")
                gr.Markdown("### âš–ï¸ æƒé‡æ··åˆ")
                
                blend_slider = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=1.0,
                    step=0.05,
                    label="æ··åˆæ¯”ä¾‹ (0=åŸºç¡€, 1=å¾®è°ƒ)",
                    info="å®æ—¶è°ƒèŠ‚åŸºç¡€æ¨¡å‹ä¸å¾®è°ƒæ¨¡å‹çš„æ··åˆç¨‹åº¦",
                )
                
                blend_btn = gr.Button("ğŸ”€ åº”ç”¨æ··åˆ")
                blend_status = gr.Textbox(label="æ··åˆçŠ¶æ€", interactive=False)
            
            # å³ä¾§ï¼šç”Ÿæˆé¢æ¿
            with gr.Column(scale=2):
                gr.Markdown("### ğŸ–¼ï¸ å›¾åƒç”Ÿæˆ")
                
                prompt_input = gr.Textbox(
                    label="Prompt",
                    placeholder="æè¿°ä½ æƒ³ç”Ÿæˆçš„å›¾åƒ...",
                    lines=3,
                )
                
                negative_prompt_input = gr.Textbox(
                    label="Negative Prompt",
                    placeholder="æè¿°ä½ ä¸æƒ³è¦çš„å…ƒç´ ...",
                    lines=2,
                )
                
                with gr.Row():
                    width_input = gr.Slider(256, 2048, 1024, step=64, label="å®½åº¦")
                    height_input = gr.Slider(256, 2048, 1024, step=64, label="é«˜åº¦")
                
                with gr.Row():
                    steps_input = gr.Slider(4, 50, 9, step=1, label="æ­¥æ•°")
                    cfg_input = gr.Slider(0.0, 15.0, 1.0, step=0.1, label="CFG Scale")
                    seed_input = gr.Number(label="Seed (-1=éšæœº)", value=-1)
                
                with gr.Row():
                    shift_input = gr.Slider(0.0, 10.0, 3.0, step=0.1, label="Shift", info="Turbo æ¨¡å‹é€šå¸¸ä½¿ç”¨ 3.0")
                
                generate_btn = gr.Button("ğŸ¨ ç”Ÿæˆå›¾åƒ", variant="primary", size="lg")
                
                output_image = gr.Image(label="ç”Ÿæˆç»“æœ", type="pil")
                gen_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False)
        
        # äº‹ä»¶ç»‘å®š
        def on_load_base(base_path, vae_path, te_path):
            try:
                load_pipeline(base_path, vae_path, te_path)
                return "âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ"
            except Exception as e:
                return f"âŒ åŠ è½½å¤±è´¥: {str(e)}"
        
        def on_scan(directory):
            models = scan_finetune_models(directory)
            if not models:
                return gr.update(choices=[], value=None)
            
            choices = [(f"{m['name']} ({m['size']})", m["path"]) for m in models]
            return gr.update(choices=choices, value=choices[0][1] if choices else None)
        
        def on_load_finetune(finetune_path):
            if not finetune_path:
                return "è¯·å…ˆé€‰æ‹©å¾®è°ƒæ¨¡å‹"
            _, msg = load_finetune_weights(finetune_path)
            return msg
        
        def on_blend(ratio):
            return blend_weights(ratio)
        
        def on_generate(prompt, neg_prompt, width, height, steps, cfg, seed, blend, shift):
            img, msg = generate_image(prompt, neg_prompt, int(width), int(height), int(steps), cfg, int(seed), blend, shift)
            return img, msg
        
        load_base_btn.click(
            on_load_base,
            inputs=[base_model_input, vae_input, text_encoder_input],
            outputs=[base_status],
        )
        
        scan_btn.click(
            on_scan,
            inputs=[finetune_dir],
            outputs=[finetune_dropdown],
        )
        
        load_finetune_btn.click(
            on_load_finetune,
            inputs=[finetune_dropdown],
            outputs=[finetune_status],
        )
        
        blend_btn.click(
            on_blend,
            inputs=[blend_slider],
            outputs=[blend_status],
        )
        
        generate_btn.click(
            on_generate,
            inputs=[
                prompt_input,
                negative_prompt_input,
                width_input,
                height_input,
                steps_input,
                cfg_input,
                seed_input,
                blend_slider,
                shift_input,
            ],
            outputs=[output_image, gen_status],
        )
    
    return demo


if __name__ == "__main__":
    demo = create_ui()
    demo.launch(
        server_name="0.0.0.0",
        server_port=3199,
        share=False,
        inbrowser=False,
    )
