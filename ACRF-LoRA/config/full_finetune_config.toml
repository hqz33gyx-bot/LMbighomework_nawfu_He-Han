# Z-Image Full Fine-tune 训练配置文件
# ==========================================
# 全量微调配置 - 训练 Transformer 的全部或部分参数
# ⚠️ 显存需求高：建议 24GB+，推荐 48GB+
# 适用于 scripts/train_full_finetune.py

[model]
# Transformer 模型路径 (必须是 .safetensors 或 diffusers 目录)
dit = "/path/to/transformer"

# 输出目录 (会自动创建)
output_dir = "output/full_finetune_v1"

# 输出文件名前缀
output_name = "zimage-finetune"

[acrf]
# Turbo 开关
enable_turbo = true

# Turbo 步数（锚点数量）
turbo_steps = 10

# 时间步 Shift 参数 (官方固定值 3.0)
shift = 3.0

# 锚点抖动幅度 (建议 0.01-0.05)
jitter_scale = 0.02

# Latent Jitter (构图突破)
latent_jitter_scale = 0.0

# Min-SNR 加权参数
snr_gamma = 5.0
snr_floor = 0.1

# RAFT L2 混合模式
raft_mode = false
free_stream_ratio = 0.3

# L2 调度参数
l2_schedule_mode = "constant"
l2_initial_ratio = 0.3
l2_final_ratio = 0.3

# 时间步感知 Loss 权重
enable_timestep_aware_loss = false
timestep_high_threshold = 0.7
timestep_low_threshold = 0.3

[finetune]
# ==========================================
# 全量微调专用配置
# ==========================================

# 可训练模块选择
# - "all"       : 全部参数（显存需求最高，效果最好）
# - "attention" : 仅注意力层（推荐，显存友好，效果好）
# - "mlp"       : 仅 MLP/FFN 层
# - "norm"      : 仅归一化层（显存最省，效果有限）
trainable_modules = "attention"

# 冻结嵌入层（推荐开启）
freeze_embeddings = true

[training]
# 优化器类型
optimizer_type = "AdamW8bit"

# 学习率 ⚠️ 全量微调需要非常小的学习率！
# 建议范围: 1e-7 ~ 1e-5
learning_rate = 1e-6

# 权重衰减
weight_decay = 0.01

# 学习率调度器
lr_scheduler = "cosine"

# Warmup 步数
lr_warmup_steps = 100

# Cosine 调度器循环次数
lr_num_cycles = 1

# 训练 Epoch 数
num_train_epochs = 5

# 保存间隔 (Epoch)
save_every_n_epochs = 1

# ==========================================
# Loss 权重配置 (与 LoRA 训练一致)
# ==========================================

# 基础损失权重
lambda_l1 = 1.0
lambda_cosine = 0.1

# 频域感知 (开关+权重+子参数)
enable_freq = false
lambda_freq = 0.3
alpha_hf = 1.0
beta_lf = 0.2

# 风格结构 (开关+权重+子参数)
enable_style = false
lambda_style = 0.3
lambda_struct = 1.0
lambda_light = 0.5
lambda_color = 0.3
lambda_tex = 0.5

[advanced]
# 梯度裁剪阈值
max_grad_norm = 1.0

# 梯度检查点（全量微调强制开启）
gradient_checkpointing = true

# 梯度累积步数
gradient_accumulation_steps = 4

# 混合精度
mixed_precision = "bf16"

# 随机种子
seed = 42

# ============ Dataset 配置 ============
[dataset]
# 批次大小
batch_size = 1

# 是否打乱数据
shuffle = true

# 是否启用分桶
enable_bucket = true

# 文本编码最大序列长度
max_sequence_length = 512

# --- 数据集列表 ---
[[dataset.sources]]
# 缓存目录路径
cache_directory = "/path/to/your/dataset/cache"
# 数据重复次数
num_repeats = 5
# 分辨率上限
resolution_limit = 1024

# ==========================================
# 使用方法
# ==========================================
# 
# 1. 修改上面的 dit 和 cache_directory 路径
# 
# 2. 选择可训练模块:
#    - trainable_modules = "all"       # 全参数，需要 48GB+
#    - trainable_modules = "attention" # 仅注意力，需要 24GB+（推荐）
# 
# 3. 运行训练:
#    accelerate launch --mixed_precision bf16 scripts/train_full_finetune.py \
#        --config config/full_finetune_config.toml
# 
# 4. 与 LoRA 训练功能一致，支持:
#    - Freq/Style Loss（enable_freq/enable_style）
#    - 时间步感知（enable_timestep_aware_loss）
#    - RAFT L2 混合模式（raft_mode）
# 
# ==========================================
